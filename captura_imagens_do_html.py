from bs4 import BeautifulSoup
import os
import requests
import re
import base64
import hashlib # Necess√°rio para gerar hashes das imagens

# O diret√≥rio onde as imagens ser√£o salvas
OUTPUT_IMAGES_DIR = "imagens_extraidas"


def capturar_imagens_do_corpo_html(caminho_html: str, caminho_imagens_destino: str):
    """
    Filtra o corpo da p√°gina HTML e captura todas as imagens, 
    ignorando duplicatas baseadas no conte√∫do (hash bin√°rio).
    Retorna um dicion√°rio com informa√ß√µes das imagens incluindo p√°gina de origem.
    """
    print("\n--- FASE B & C: Filtragem do Corpo e Captura de Imagens ---")
    
    if not os.path.exists(caminho_imagens_destino):
        os.makedirs(caminho_imagens_destino)
        
    try:
        with open(caminho_html, 'r', encoding='utf-8') as f:
            html_content = f.read()
    except FileNotFoundError:
        print(f"‚ùå Erro: Arquivo HTML n√£o encontrado em {caminho_html}")
        return False

    soup = BeautifulSoup(html_content, 'html.parser')
    imagens_salvas = []
    imagens_info = {}  # Dicion√°rio para armazenar informa√ß√µes das imagens
    
    # --- NOVO: CONJUNTO PARA ARMAZENAR HASHES DE IMAGENS J√Å VISTAS ---
    hashes_salvos = set()
    contador_imagens_unicas = 0 # Usado para nomear as imagens sequencialmente
    
    body = soup.find('body')
    if not body:
        print("‚ùå N√£o foi poss√≠vel encontrar a tag <body> no HTML.")
        return False

    # Vari√°vel para rastrear o total de imagens encontradas (para a mensagem final)
    total_imagens_encontradas = 0

    # 1. Iterar sobre todas as tags de imagem
    for i, img in enumerate(body.find_all('img')):
        src = img.get('src')
        page_number = img.get('data-page-number', 'Desconhecida')  # Extrair n√∫mero da p√°gina
        if not src:
            continue
        
        total_imagens_encontradas += 1
        img_data = None 

        # 2. L√≥gica de Captura (Obt√©m os dados bin√°rios)
        if src.startswith('data:image'):
            # A. Imagem embutida em Base64
            try:
                header, encoded = src.split(',', 1)
                img_data = base64.b64decode(encoded)
            except Exception:
                continue
            
        elif src.startswith(('http', 'https')):
            # B. Imagem externa (apenas para completude)
            try:
                img_data = requests.get(src).content
            except Exception:
                continue

        # --- FILTRO DE DUPLICATAS POR HASH ---
        if img_data:
            # 3. Gerar o Hash do Conte√∫do Bin√°rio
            img_hash = hashlib.sha256(img_data).hexdigest()
            
            if img_hash in hashes_salvos:
                # SE O HASH EXISTE, ESTA IMAGEM √â UMA DUPLICATA E √â IGNORADA.
                print(f"   ‚ÑπÔ∏è Imagem {i+1} ignorada: Duplicata (Hash: {img_hash[:6]}).")
                continue
            
            # --- NOVO FILTRO DE TAMANHO M√çNIMO (RELEV√ÇNCIA) ---
            # Define o limite m√≠nimo (ex: 5 KB). Imagens pequenas (logotipos, linhas) ser√£o descartadas.
            MIN_SIZE_BYTES = 7500 
            
            if len(img_data) < MIN_SIZE_BYTES:
                 print(f"   ‚ÑπÔ∏è Imagem {i+1} ignorada: Tamanho {len(img_data)}B √© muito pequeno para ser um diagrama (min: 7.500 bytes).")
                 # O HASH n√£o √© adicionado, permitindo que a pr√≥xima ocorr√™ncia (se for maior) seja verificada.
                 continue
            
            # --- SE CHEGOU AQUI, A IMAGEM √â √öNICA ---
            
            # 4. Adiciona o hash ao conjunto (garantindo que futuras c√≥pias sejam ignoradas)
            hashes_salvos.add(img_hash)
            
            # 5. Incrementa o contador de imagens √öNICAS
            contador_imagens_unicas += 1

            # 6. Salvar o arquivo com um nome sequencial baseado apenas nas imagens √öNICAS
            nome_arquivo = f"img_{contador_imagens_unicas}.png"
            caminho_saida = os.path.join(caminho_imagens_destino, nome_arquivo)

            with open(caminho_saida, 'wb') as f:
                f.write(img_data)
            
            # 7. Armazenar informa√ß√µes da imagem incluindo p√°gina
            imagens_info[nome_arquivo] = {
                'caminho': caminho_saida,
                'pagina': page_number,
                'hash': img_hash
            }
            
            imagens_salvas.append(caminho_saida)
            print(f"   ‚úÖ Imagem √∫nica salva como: {nome_arquivo} (P√°gina {page_number})")

    print(f"‚úÖ Captura conclu√≠da. {len(imagens_salvas)} imagens √∫nicas salvas em {caminho_imagens_destino}")
    print(f"   Foram encontradas e descartadas {total_imagens_encontradas - len(imagens_salvas)} imagens repetidas (incluindo a primeira c√≥pia).")
    
    # Salvar informa√ß√µes das imagens em um arquivo JSON
    import json
    info_file_path = os.path.join(caminho_imagens_destino, 'imagens_info.json')
    with open(info_file_path, 'w', encoding='utf-8') as f:
        json.dump(imagens_info, f, ensure_ascii=False, indent=2)
    
    print(f"   üìÑ Informa√ß√µes das imagens salvas em: {info_file_path}")
    
    return True